#-*- coding: utf-8 -*-
"""gans01_pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15UwRZ7o9GLcCg0FuYrVT9x1lhL0vkbhL
"""

# ~~~~~~~~~~~~~~~~~~~~ Packages ~~~~~~~~~~~~~~~~~~~~
import cv2
import os
from os import listdir
import sys
import argparse
from posixpath import join
import shutil

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy

import face_recognition as fr
import imquality.brisque as brisque

import imutils
import time
from imutils.object_detection import non_max_suppression
#from google.colab.patches import cv2_imshow

from datetime import datetime
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import PIL
from PIL import Image, ImageStat
# ~~~~~~~~~~~~~~~~~~~~ Packages ~~~~~~~~~~~~~~~~~~~~

# ~~~~~~~~~~~~~~~~~~~~ Pathing ~~~~~~~~~~~~~~~~~~~~
prefix = '/home/hume-users/leebri2n/Documents/'
# modify customized_path
proj_path = os.path.join(os.path.join(prefix, 'hume2022'), 'eshaep_gans')
data_path = os.path.join(prefix, 'data')

print('Path to project files: {}'.format(proj_path))
print('Path to data files: {}'.format(data_path))
# ~~~~~~~~~~~~~~~~~~~~ Pathing ~~~~~~~~~~~~~~~~~~~~

# ~~~~~~~~~~~~~~~~~~~~ Class ~~~~~~~~~~~~~~~~~~~~
class Pipeline():
    def __init__(self, proj_path='', data_path ='', input_folder='input', output_folder='output', \
               size=300, blur_thresh=30, text_thresh=0.85, qual_thresh=70) -> None:
        #Simple paths
        self.proj_path = proj_path
        self.data_path = data_path
        self.input_path = os.path.join(data_path, input_folder)
        self.output_path = os.path.join(data_path, output_folder)

        #Default conditions
        self.valid_ext = ['.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp', '.gif',\
                   '.eps', '.raw', '.cr2', '.nef', '.orf', '.sr2','.webp']
        self.size = size
        self.blur_thresh = blur_thresh
        self.text_thresh = text_thresh
        self.blurry_input = []
        self.text_input = []

        #Storage
        self.allfiles_input = [] #directory paths
        self.imgfiles_input = [] #directory paths
        self.acc_img = [] #directory paths
        self.rej_img = [] #directory paths
        self.display_img = [] #PIL Image objects

        #Logging
        self.input_widths = [] #int
        self.input_heights = [] #int
        self.input_formats = set() #str
        self.img_dict = dict()

    def filter(self, input_path, output_path, size):
        """
          Given input data files, processes and sorts them according to usability.
          Main data pipeline function.

          Params:
            input_path: Directory path to input images.
            output_path: Directory path to output images.
            size: Desired length of square output image's edge.
        """
        self.walk(self.input_path, self.allfiles_input)
        print("NUMBER OF POTENTIAL INPUTS: ", str(len(self.allfiles_input)))

        img_num = 0
        for img in self.allfiles_input:
            # ~~~~~~~~~~~~~~~~ Start Loop ~~~~~~~~~~~~~~~~
            cur_name = os.path.basename(os.path.normpath(img))
            print("Assessing image", str(img_num), " of ", \
            str(len(self.allfiles_input)), cur_name)
            if self.reject_image(img, self.input_path, output_path, size): continue

            out_name = str(img_num).zfill(6)+'.png'
            print(cur_name, "ACCEPTED as", out_name)

            #assemble dict ~~~
            self.add_entry(img, out_name, self.img_dict)
            acc_folder = os.path.join(output_path, "accepted")
            acc_img_path = os.path.join(acc_folder, out_name)

            #Image handling ~~~
            img_standard = self.img_standardize(img, size)
            self.display_img.append(img_standard)
            img_standard.save(acc_img_path)

            #Path handling ~~~
            self.acc_img.append(acc_img_path)

            #Update valid img number
            img_num += 1
            # ~~~~~~~~~~~~~~~~ End Loop ~~~~~~~~~~~~~~~~

        #save to json to /output ~~~
        self.writejson(self.output_path)

        #save log file
        self.writelog(self.imgfiles_input, self.input_widths, self.input_heights,
                 self.input_formats, len(self.acc_img))

        # Peek at cropped images
        self.display_accepted(self.display_img)

        # Returns dictionary
        return self.img_dict

#~~~~~~~~~~~~ Helper functions ~~~~~~~~~~~~~~~~~
    def reject_image(self, cur_img, input_path, output_path, size):
        """
        All-encompassing function handling unusable images. Desired values for
        various thresholds can be tweaked. See: valid extensions, dimension minima,
        and blur threshold.

        Params:
          cur_img: The image to be examined.
          input_path: Directory path to input image folder.
          output_path: Directory path to output image folder.
          size: Desired size for which to qualify minimum size requirements.
          img_blur_list: The list of image paths classified as blurry.
          img_text_list: The list of image paths classified with detected text.

        Returns: Boolean
          True if the image should be rejected, and False if the image can be used.
        """
        reject_path = os.path.join(output_path, "rejected")
        cur_name = os.path.basename(os.path.normpath(cur_img))

        #Criteria 1: Is an image
        cur_ext = os.path.splitext(cur_img)[1]
        self.input_formats.add(cur_ext)
        start = time.time()
        if cur_ext not in self.valid_ext:
            print(cur_name, " REJECTED.", " Warning: Not an image.")
            shutil.copy(cur_img, reject_path)
            os.rename(os.path.join(reject_path, cur_name), \
                os.path.join(reject_path, 'rjnotimg_'+cur_name))
            return True
        end = time.time()
        #print(cur_name, "TIME FOR FORMAT ELAPSED: ", str(end-start))

        #Criteria 2: Sufficiently high resolution
        start = time.time()
        img = Image.open(cur_img)
        width, height = img.size

        #Is a valid image, potentially rejectable
        self.imgfiles_input.append(cur_img)
        self.input_widths.append(width)
        self.input_heights.append(height)
        if (width <= 0.8*size) or (height <= int(0.8*size)):
            print(cur_name, " REJECTED.", " Warning: Resolution too low.")
            shutil.copy(cur_img, reject_path)
            os.rename(os.path.join(reject_path, cur_name), \
                os.path.join(reject_path, 'rjres_'+cur_name))
            return True
        end = time.time()
        #print(cur_name, "TIME FOR RESOLUTION ELAPSED: ", str(end-start))

        #Criteria 3: Is not in file path "blurry"
        start = time.time()
        if cur_name in self.blurry_input:
            print(cur_name, " REJECTED.", " Warning: Too blurry.")
            shutil.copy(cur_img, reject_path)
            os.rename(os.path.join(reject_path, cur_name), \
                os.path.join(reject_path, 'rjblurry_'+cur_name))
            return True
        end = time.time()
        #print(cur_name, "TIME FOR BLURRY ELAPSED: ", str(end-start))

        #Criteria 6: Grayscale image
        start = time.time()
        img_info = cv2.imread(cur_img)
        if len(img_info.shape) < 2:
            print(cur_name, "REJECTED.", "Warning: Grayscale image.")
            shutil.copy(cur_img, reject_path)
            os.rename(os.path.join(reject_path, cur_name), \
                os.path.join(reject_path, 'rjgrayscale_'+cur_name))
            return True
        end = time.time()
        #print(cur_name, "TIME FOR GRAYSCALE ELAPSED: ", str(end-start))

        #Criteria 4: No human faces
        start = time.time()
        cur_img_fr = fr.load_image_file(cur_img)
        face_landmarks = fr.face_locations(cur_img_fr, model='hog')
        if len(face_landmarks) != 0:
            print(cur_name," REJECTED.", " Warning: Faces detected.")
            shutil.copy(cur_img, reject_path)
            os.rename(os.path.join(reject_path, cur_name), \
              os.path.join(reject_path, 'rjface_'+cur_name))
            return True
        end = time.time()
        print(cur_name, "TIME FOR FACE ELAPSED: ", str(end-start))

        #Criteria 7: Text
        start = time.time()
        if cur_name in self.text_input:
            print(cur_name, "REJECTED.", "Warning: Text detected.")
            shutil.copy(cur_img, reject_path)
            os.rename(os.path.join(reject_path, cur_name), \
              os.path.join(reject_path, 'rjtext_'+cur_name))
            return True
        end = time.time()
        print(cur_name, "TIME FOR TEXT ELAPSED: ", str(end-start))

        #Usable image
        return False

    def walk(self, input_path, file_list):
        """
        Searches for and collects directory pathways to files.

        Params:
        input_path: Directory path to input images.
        files: A list to store directory paths to valid files.

        Returns:????
        files: A list of pathways to valid files.
        """
        input_list = os.listdir(input_path)
        for item in input_list:
            if item == 'classifications':
                continue

            if os.path.isdir(os.path.join(input_path,item)):
                self.walk(os.path.join(input_path, item), file_list)
            else:
                file_list.append(os.path.join(input_path, item))

        return file_list

    def display_accepted(self, displayimg):
        """
          Displays images that passed the acceptance criteria.

          Params:
            displayimg: List of Image objects to plot.
        """
        plt.figure(figsize=(10,10))
        for i in range(len(displayimg)):
          plt.subplot(len(displayimg), len(displayimg), i+1)
          plt.axis('off')
        plt.show()

    def img_standardize(self, cur_img, size):
        """
          A function that crops and resizes the image being examined.

          Params:
            cur_img: The image being standardized.
            size: The desired size to which to resize the input image to.
          Returns:
            img: PIL Image object of the standardized image.
        """
        img = Image.open(cur_img)
        width, height = img.size

        if (width > height):
            left_start = (width-height)/2
            img = img.crop((left_start, 0, left_start+height,height))
        elif (height > width):
            top_start = (height-width)/2
            img = img.crop((0, top_start, width, top_start+width))

        img = img.resize((size,size))
        return img

    def blur_detection(self, input_path, thresh=30, split=True, v=True):
        """
          A function that detects and designates images as having an unacceptable
          amount of blurring.

          Params:
            input_path: Directory path to input images.
            thresh: Blurring threshold at which to classify as "blurry."
            split: Boolean denoting whether or nto to copy blurry images into a
              "blurry" subdirectory.
            verbose: Track progress verbosely.

          Returns:
            A list of image NAMES classified as blurry. MAKE PATHS?
        """
        class_path = os.path.join(input_path, 'classifications')
        blurry_path = os.path.join(class_path, 'blurry')
        sharp_path = os.path.join(class_path, 'sharp')

        input_list = []
        self.walk(input_path, input_list)
        input_list.sort()

        if split:
            shutil.rmtree(blurry_path)
            shutil.rmtree(sharp_path)
            os.makedirs(blurry_path, exist_ok=True)
            os.makedirs(sharp_path, exist_ok=True)

        print("Assembling list of blurred images...")
        lpcs = []
        for img_path in input_list:
            cur_name = os.path.basename(os.path.normpath(img_path))
            if v:
                print(cur_name, img_path)
            try:
                img_cv2 = cv2.imread(img_path)
                img_gray = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2GRAY)
                lpc = cv2.Laplacian(img_gray, cv2.CV_64F).var()
                lpcs.append(lpc)

                if v: print(lpc)
                if split:
                    if lpc >= thresh:
                        cv2.imwrite(os.path.join(sharp_path, cur_name), img_cv2)
                    else:
                        cv2.imwrite(os.path.join(blurry_path, cur_name), img_cv2)
            except:
                if v: print("NOTE: " + cur_name + " is not an image. Continuing.")

        #return lpcs
        return os.listdir(blurry_path)

    def text_detection(self, input_list, confidence, allow=1, resize=640):
        """
          Detects and recognizes allged text that appears in images.

          Params:
            cur_img: Directory path of the current image being assessed.
            confidence: Probability as a measure of sensitivity to which text
            is detected in an image.

          Returns:
            The number of distinct text objects recognized.
        """
        class_path = os.path.join(input_path, 'classifications')
        text_path = os.path.join(class_path, 'textdetection')
        print(text_path)
        shutil.rmtree(text_path)
        os.makedirs(text_path, exist_ok=True)

        input_list = []
        self.walk(input_path, input_list)
        input_list.sort()
        model_path = os.path.join(self.proj_path, 'frozen_east_text_detection.pb')

        print("Identifying text in images...")
        #Loop thru input_list, detect text
        for cur_img in input_list:
            cur_name = os.path.basename(os.path.normpath(cur_img))
            print(cur_name)
            wid_new = resize
            hei_new = resize #Adjustable

            try:
                img = cv2.imread(cur_img)
                img_orig = img.copy()
            except:
                continue

            (hei_orig, wid_orig) = img.shape[:2]
            wid_ratio = wid_orig / float(wid_new)
            hei_ratio = hei_orig / float (hei_new)

            img = cv2.resize(img, (wid_new, hei_new))
            #Grab dimensions?

            layerNames = ["feature_fusion/Conv_7/Sigmoid",
                "feature_fusion/concat_3"]

            net = cv2.dnn.readNet(model_path)
            blob = cv2.dnn.blobFromImage(img, 1.0, (wid_new, hei_new), \
                (123.68, 116.78, 103.94), swapRB=True, crop=False)
            net.setInput(blob)
            (scores, geometry) = net.forward(layerNames)

            (numRows, numCols) = scores.shape[2:4]
            rects = []
            confidences = []

            for y in range(0, numRows):
                # extract the scores (probabilities), followed by the geometrical data
                scoresData = scores[0, 0, y]
                xData0 = geometry[0, 0, y]
                xData1 = geometry[0, 1, y]
                xData2 = geometry[0, 2, y]
                xData3 = geometry[0, 3, y]
                anglesData = geometry[0, 4, y]

                # loop over the number of columns
                for x in range(0, numCols):
                    # if our score does not have sufficient probability, ignore it
                    if scoresData[x] < confidence:
                        continue
                    # compute the offset factor as our resulting feature maps will 4x smaller
                    (offsetX, offsetY) = (x * 4.0, y * 4.0)
                    # extract the rotation angle for the prediction and then compute sin, cos
                    angle = anglesData[x]
                    cos = np.cos(angle)
                    sin = np.sin(angle)
                    # use the geometry volume to derive the width and height of pred box
                    h = xData0[x] + xData2[x]
                    w = xData1[x] + xData3[x]
                    # compute both the starting and ending (x, y)-coordinates for pred box
                    endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))
                    endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))
                    startX = int(endX - w)
                    startY = int(endY - h)
                    # add the bounding box coordinates and probability score to resp. lists
                    rects.append((startX, startY, endX, endY))
                    confidences.append(scoresData[x])

            boxes = non_max_suppression(np.array(rects), probs=confidences)
            for (startX, startY, endX, endY) in boxes:
                # scale the bounding box coordinates based on the respective ratios
                startX = int(startX * wid_ratio)
                startY = int(startY * hei_ratio)
                endX = int(endX * wid_ratio)
                endY = int(endY * hei_ratio)
                # draw the bounding box on the image
                cv2.rectangle(img_orig, (startX, startY), (endX, endY), (0, 255, 0), 2)

            #cv2.imshow(img_orig)
            if len(rects) > allow:
                print(len(rects))
                cv2.imwrite(os.path.join(text_path, cur_name), img_orig)

        return os.listdir(text_path)

    def writejson(self, output_path):
        try:
            metadata = open(os.path.join(output_path, 'input_metadata.json'), 'w')
            json.dump(self.img_dict, metadata, indent=1)
            metadata.close()
            print('input_metadata.json written successfully.')
        except:
            print('Unable to write input_metadata.json.')

    def writelog(self, imgfiles_inputlist, widths, heights, formats, acc):
        """
          Writes a text file containing statistics about the input dataset.

          Params:
            imgfiles_inputlist: list of valid input image paths
            widths: list of image widths
            heights: list of image heights
            formats: set of image formats that appear
            acc: number of accepted images
        """
        datetime_exc = datetime.now() #Date and time of finished execution
        num = len(imgfiles_inputlist)
        minw = np.min(widths)
        maxw = np.max(widths)
        minh = np.min(heights)
        maxh = np.max(heights)
        avw = np.average(widths)
        avh = np.average(heights)
        rej = num - acc
        percentage = (acc / (acc + rej))

        #Write stuff to file
        log_path = os.path.join(self.proj_path, 'logs')
        with open(os.path.join(log_path, \
            (datetime_exc.strftime("%m%d%Y_%H:%M:%S")))+'log.txt', 'w') as f:
            f.write(join("Latest successful execution:", datetime_exc.strftime("%m/%d/%Y, %H:%M:%S")))
            f.write('\n')

            f.write(join("Number of valid images:", str(num)))
            f.write('\n')
            f.write('\n')

            f.write(join('Minimum input width:', str(minw)))
            f.write('\n')
            f.write(join('Maximum input width:', str(maxw)))
            f.write('\n')
            f.write('\n')

            f.write(join('Minimum input height:', str(minh)))
            f.write('\n')
            f.write(join('Maximum input height:', str(maxh)))
            f.write('\n')
            f.write('\n')

            f.write(join('Average input width:', str(avw)))
            f.write('\n')
            f.write(join('Average input height:', str(avh)))
            f.write('\n')
            f.write('\n')

            f.write(join('Percentage of accepted images:', join(str(percentage*100), '%')))
            print('log.txt successfully written.')

    def add_entry(self, img, img_outname, img_dict):
        """
          Adds metadata entry to the metadictionary to be written.

          Params:
            img_dict: The dictionary to which to store metadata in.
            img: The directory path to the image file.
            img_outname: The accepted image's output file name.
        """
        res = Image.open(img).size
        ext = os.path.splitext(img)[1]
        par = os.path.abspath(os.path.join(img, os.pardir))
        self.img_dict[img_outname]=dict()
        self.img_dict[img_outname]['original_resolution'] = res
        self.img_dict[img_outname]['original_format'] = ext
        self.img_dict[img_outname]['original_path'] = par


#~~~~~~~~~~~~~~~~~~ Execution ~~~~~~~~~~~~~~~~~~~
input_path = os.path.join(data_path, 'input')
output_path = os.path.join(data_path, 'output')

pipeline = Pipeline(proj_path=proj_path, input_folder=input_path, output_folder=output_path, \
    size=1024, blur_thresh=30, text_thresh=0.99)

pipeline.blurry_input = pipeline.blur_detection(input_path, v=False)
print(pipeline.blurry_input)
pipeline.text_input = pipeline.text_detection(input_path, confidence=0.99, allow=3)
print(pipeline.text_input)

start = time.time()
#pipeline.filter(input_path, output_path, pipeline.size)
end = time.time()

print("FILTERING EXECUTION TIME: ", str((end-start)/60), "MINUTES")
